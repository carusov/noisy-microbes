---
title: "dada2_pipeline"
author: "Vincent Caruso"
date: "July 20, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Setup

First, load required libraries.
```{r libraries}

library("dada2")
library("stringr")
library("ggplot2")

```


##Set up working directories

Get working directory from command line
```{r}

library(optparse)
 
option_list = list(
  make_option(c("-i", "--input"), type = "character", default = NULL, 
              help = "data directory", metavar = NULL),
  make_option(c("-o", "--output"), type = "character", default = NULL,
              help = "results directory", metavar = NULL),
  make_option(c("-f", "--ftrunc"), type = "integer", default = 230,
              help = "forward read truncate position"),
  make_option(c("-b", "--rtrunc"), type = "integer", default = 210,
              help = "reverse read truncate position"),
  make_option(c("-s", "--min_len"), type = "integer", default = 230, 
              help = "minimum merged length"),
  make_option(c("-l", "--max_len"), type = "integer", default = 235,
              help = "maximum merged length")
)
 
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)

```

Next, define the working directory and file paths.
```{r paths}

data_path <- "~/thesis/data/dilution"
data_path <- opt$input     # parent directory for raw and filtered data
# result_path <- "~/thesis/results/dilution"
dada2_path <- opt$output    # directory for outputs of DADA2 read processsing
dada2_path <- "~/thesis/results/dilution/dada2"    # directory for outputs of DADA2 read processsing
#ref_path <- "~/projects/thesis/references"    # directory containing reference databases
raw_path <- file.path(data_path, "raw")     # directory containing raw read files
trunc_path <- file.path(data_path, "truncated")     # directory containing raw read files
#dada2_path <- file.path(result_path, "dada2")     # directory where DADA2 processing results will be stored
dada2_filt_path <- file.path(dada2_path, "filtered")     # directory where filtered reads will be stored
filt_path <- file.path(data_path, "filtered")     # directory where filtered reads will be stored

if (!file_test("-d", dada2_path)) dir.create(dada2_path)
if (!file_test("-d", dada2_path)) dir.create(dada2_path, recursive = TRUE)
if (!file_test("-d", filt_path)) dir.create(filt_path)

```


##Filtering and Trimming

Get raw data file names, split them into forward and reverse read files, and infer sample names from the file names.
```{r plot qualities}

# forward and reverse reads (truncated but not filtered)
# fastqs_paired <- list.files(raw_path)
fastqs_paired <- list.files(trunc_path)
fastqs_paired <- str_subset(fastqs_paired, ".fastq$")
fastq_Fs <- str_subset(fastqs_paired, "_R1")
fastq_Rs <- str_subset(fastqs_paired, "_R2")

# truncated, merged, and filtered reads
filt_merged <- list.files(filt_path)
filt_merged <- str_subset(filt_merged, "^s\\d{3}.+\\.fastq$")

#get the sample names
sample_names <- sapply(str_split(fastq_Fs, "_trunc_R\\d"), `[`, 1)

```

The paired reads have alread been trimmed at position 230 for the forward reads and position 210 for the reverse reads. The first 15 nucleotides-- the Illumina "burn-in"-- were also trimmed from both forward and reverse reads. So all that's left to do is filter based on maximum expected errors. The merged reads have alread been quality filtered.
```{r filter}

# Define file names for the filtered reads
filt_Fs <- paste0(sample_names, "_R1_filt.fastq")
filt_Rs <- paste0(sample_names, "_R2_filt.fastq")

# Filter paired read sets
filt_stats <- filterAndTrim(fwd = file.path(trunc_path, fastq_Fs), filt = file.path(dada2_filt_path, filt_Fs),
                            rev = file.path(trunc_path, fastq_Rs), filt.rev = file.path(dada2_filt_path, filt_Rs),
                            # truncLen = c(230, 210), trimLeft = 15, maxEE = c(3, 4), truncQ = 2, rm.phix = TRUE,
                            maxEE = c(3, 4), #truncLen = c(opt$ftrunc, opt$rtrunc), trimLeft = 15, truncQ = 2, rm.phix = TRUE,
                            compress = TRUE, verbose = TRUE, multithread = TRUE)

```


##Error parameter estimation

Learn the error rates from the data.
```{r errors}

err_F <- learnErrors(file.path(dada2_filt_path, filt_Fs), multithread = TRUE)
err_R <- learnErrors(file.path(dada2_filt_path, filt_Rs), multithread = TRUE)
err_M <- learnErrors(file.path(filt_path, filt_merged), multithread = TRUE)

```


##Dereplication

Collapse sequence replicates into single sequences, each with a summary of the quality scores at each base position.
```{r dereplicate}

derep_Fs <- derepFastq(file.path(dada2_filt_path, filt_Fs), verbose = TRUE)
derep_Rs <- derepFastq(file.path(dada2_filt_path, filt_Rs), verbose = TRUE)
derep_Ms <- derepFastq(file.path(filt_path, filt_merged), verbose = TRUE)

#names(derep_Fs) <- sample_names
#names(derep_Rs) <- sample_names

```


##Inference of sequence variants

Since I used all the read data to learn the error rates, the sequence inference for all samples has already been done by DADA2. However, I'll do the inference again formally here, for consistency, using the learned error rates.
```{r SV inference}

dada_Fs <- dada(derep_Fs, err = err_F, multithread = TRUE)
dada_Rs <- dada(derep_Rs, err = err_R, multithread = TRUE)
dada_Ms <- dada(derep_Ms, err = err_M, multithread = TRUE)

# Save the dada objects
save(err_F, err_R, err_M, derep_Fs, derep_Rs, derep_Ms, dada_Fs, dada_Rs, dada_Ms, file = file.path(dada2_path, "dada2.RData"))

```


##Merging of paired reads

Like it says, now I'm going to merge the paired reads. This will reduce the number of spurious sequences.
```{r merge SVs}

#load(file = file.path(dada2_path, "dada2.RData"))
mergers <- mergePairs(dada_Fs, derep_Fs, dada_Rs, derep_Rs, 
                     propagateCol = c("n0", "n1", "birth_fold", "birth_ham"), 
                     verbose = TRUE)

```


##Create a sequence table

This converts the inferred sequence data into a table, similar to an OTU table.
```{r sequence table}

sv_table_FR <- makeSequenceTable(mergers)
sv_table_M <- makeSequenceTable(dada_Ms)
row.names(sv_table_FR) <- sample_names
row.names(sv_table_M) <- sample_names

print("Paired read sequence lengths:")
table(nchar(getSequences(sv_table_FR)))
print("Merged read sequence lengths:")
table(nchar(getSequences(sv_table_M)))

```

If there are any sequences with lengths outside the expected range for the V4 region, I want to remove those.
```{r remove bad lengths}

min_len <- opt$min_len
max_len <- opt$max_len
sv_table_FR <- sv_table_FR[, nchar(getSequences(sv_table_FR)) %in% seq(min_len, max_len)]
sv_table_M <- sv_table_M[, nchar(getSequences(sv_table_M)) %in% seq(min_len, max_len)]

print("Paired read sequence lengths after length filtering:")
table(nchar(getSequences(sv_table_FR)))
print("Merged read sequence lengths after length filtering:")
table(nchar(getSequences(sv_table_M)))

```


##Remove chimeras

DADA2 only considers "bimeras", or chimeras spawned from exactly two parents sequences.
```{r remove chimeras}

sv_table_FR.no_chim <- removeBimeraDenovo(sv_table_FR, method = "consensus", verbose = TRUE)
sv_table_M.no_chim <- removeBimeraDenovo(sv_table_M, method = "consensus", verbose = TRUE)

#check what percentage of reads remain
print("Proportion of paired reads remaining after bimera removal:")
sum(sv_table_FR.no_chim) / sum(sv_table_FR)
print("Proportion of merged reads remaining after bimera removal:")
sum(sv_table_M.no_chim) / sum(sv_table_M)

```


##Track read retention through the pipeline

See how many reads were retained or discarded at each stage of processing.
```{r track reads}

getN <- function(x) sum(getUniques(x))

if (length(sample_names) > 1){
  track_table_FR <- cbind(filt_stats, sapply(dada_Fs, getN), sapply(mergers, getN), rowSums(sv_table_FR), rowSums(sv_table_FR.no_chim))
} else {
  track_table_FR <- cbind(filt_stats, getN(dada_Fs), getN(mergers), sum(sv_table_FR), sum(sv_table_FR.no_chim))
}

colnames(track_table_FR) <- c("raw", "filtered", "denoised", "merged", "tabled", "non_chim")
rownames(track_table_FR) <- sample_names
print("Table of read counts through paired read pipeline:")
track_table_FR


filt_M_reads <- sapply(filt_merged, function(f) {
  n <- system2(command = "wc", args = c("-l", file.path(filt_path, f)), stdout = TRUE)
  n <- str_split(n, "\\s+")[[1]][1] %>% as.integer(.) / 4
  return(n)
})

if (length(sample_names) > 1){
  track_table_M <- cbind(filt_stats[, "reads.in"], filt_M_reads, sapply(dada_Ms, getN), rowSums(sv_table_M), rowSums(sv_table_M.no_chim))
} else {
  track_table_M <- cbind(filt_stats[, "reads.in"], filt_M_reads, getN(dada_Ms), sum(sv_table_M), sum(sv_table_M.no_chim))
}

colnames(track_table_M) <- c("raw", "filtered", "denoised", "tabled", "non_chim")
rownames(track_table_M) <- sample_names
print("Table of read counts through paired read pipeline:")
track_table_M

save(mergers, sv_table_FR, sv_table_M, sv_table_FR.no_chim, sv_table_M.no_chim,
     track_table_FR, track_table_M, file = file.path(dada2_path, "tables.RData"))
write.table(sv_table_FR.no_chim, file = file.path(dada2_path, "sv_table_FR.no_chim.txt"), quote = FALSE, sep = "\t")
write.table(sv_table_M.no_chim, file = file.path(dada2_path, "sv_table_M.no_chim.txt"), quote = FALSE, sep = "\t")

```

