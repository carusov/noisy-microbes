---
title: "dada2_pipeline"
author: "Vincent Caruso"
date: "July 20, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Setup

First, load required libraries.
```{r libraries}

library("dada2")
library("stringr")
library("ggplot2")

```


##Set up working directories

Get working directory from command line
```{r}

library(optparse)
 
option_list = list(
  make_option(c("-i", "--input"), type = "character", default = NULL, 
              help = "data directory", metavar = NULL),
  make_option(c("-o", "--output"), type = "character", default = NULL,
              help = "results directory", metavar = NULL),
  make_option(c("-f", "--ftrunc"), type = "integer", default = 230,
              help = "forward read truncate position"),
  make_option(c("-b", "--rtrunc"), type = "integer", default = 210,
              help = "reverse read truncate position"),
  make_option(c("-s", "--min_len"), type = "integer", default = 230, 
              help = "minimum merged length"),
  make_option(c("-l", "--max_len"), type = "integer", default = 235,
              help = "maximum merged length")
)
 
opt_parser = OptionParser(option_list=option_list)
opt = parse_args(opt_parser)

```

Next, define the working directory and file paths.
```{r paths}

#data_path <- "~/projects/thesis/data/test_bal"
data_path <- opt$input     # parent directory for raw and filtered data
#result_path <- "~/projects/thesis/results/test_bal"
result_path <- opt$output    # parent directory for outputs of read processsing methods
#ref_path <- "~/projects/thesis/references"    # directory containing reference databases
raw_path <- file.path(data_path, "raw")     # directory containing raw read files
dada2_path <- file.path(result_path, "dada2")     # directory where DADA2 processing results will be stored
filt_path <- file.path(dada2_path, "filtered")     # directory where filtered reads will be stored

if (!file_test("-d", dada2_path)) dir.create(dada2_path)
if (!file_test("-d", filt_path)) dir.create(filt_path)

```


##Filtering and Trimming

Get raw data file names, split them into forward and reverse read files, and infer sample names from the file names.
```{r plot qualities}

#setwd(raw_path)
file_names <- list.files(raw_path)
fastqs <- str_subset(file_names, ".fastq$")
fastq_Fs <- str_subset(fastqs, "_R1")
fastq_Rs <- str_subset(fastqs, "_R2")

#get the sample names
sample_names <- sapply(str_split(fastq_Fs, "_R\\d"), `[`, 1)

```

The quality isn't great for most of the read sets. For the "Neat" sample, I'll trim the forward reads at position 230 for now, and the reverse reads at position 210. I'm also trimming the first 10 nucleotides-- the Illumina "burn-in".
```{r filter}

# Define file names for the filtered reads
filt_Fs <- paste0(sample_names, "_R1_filt.fastq")
filt_Rs <- paste0(sample_names, "_R2_filt.fastq")

# Filter paired read sets
filt_stats <- filterAndTrim(fwd = file.path(raw_path, fastq_Fs), filt = file.path(filt_path, filt_Fs),
                            rev = file.path(raw_path, fastq_Rs), filt.rev = file.path(filt_path, filt_Rs),
                            #truncLen = c(240, 220), trimLeft = 15, maxEE = c(3, 4), truncQ = 2, rm.phix = TRUE, 
                            truncLen = c(opt$ftrunc, opt$rtrunc), trimLeft = 15, maxEE = c(3, 4), truncQ = 2, rm.phix = TRUE, 
                            compress = TRUE, verbose = TRUE, multithread = TRUE)

```


##Error parameter estimation

Learn the error rates from the data.
```{r errors}

err_F <- learnErrors(file.path(filt_path, filt_Fs), multithread = TRUE)
err_R <- learnErrors(file.path(filt_path, filt_Rs), multithread = TRUE)

```


##Dereplication

Collapse sequence replicates into single sequences, each with a summary of the quality scores at each base position.
```{r dereplicate}

derep_Fs <- derepFastq(file.path(filt_path, filt_Fs), verbose = TRUE)
derep_Rs <- derepFastq(file.path(filt_path, filt_Rs), verbose = TRUE)

#names(derep_Fs) <- sample_names
#names(derep_Rs) <- sample_names

```


##Inference of sequence variants

Since I used all the read data to learn the error rates, the sequence inference for all samples has already been done by DADA2. However, I'll do the inference again formally here, for consistency, using the learned error rates.
```{r SV inference}

dada_Fs <- dada(derep_Fs, err = err_F, multithread = TRUE)
dada_Rs <- dada(derep_Rs, err = err_R, multithread = TRUE)

# Save the dada objects
save(err_F, err_R, derep_Fs, derep_Rs, dada_Fs, dada_Rs, file = file.path(dada2_path, "dada2.RData"))

```


##Merging of paired reads

Like it says, now I'm going to merge the paired reads. This will reduce the number of spurious sequences.
```{r merge SVs}

#load(file = file.path(dada2_path, "dada2.RData"))
mergers <- mergePairs(dada_Fs, derep_Fs, dada_Rs, derep_Rs, 
                     propagateCol = c("n0", "n1", "birth_fold", "birth_ham"), 
                     verbose = TRUE)

```


##Create a sequence table

This converts the inferred sequence data into a table, similar to an OTU table.
```{r sequence table}

sv_table <- makeSequenceTable(mergers)
row.names(sv_table) <- sample_names
table(nchar(getSequences(sv_table)))

```

If there are any sequences with lengths outside the expected range for the V4 region, I want to remove those.
```{r remove bad lengths}

min_len <- opt$min_len
max_len <- opt$max_len
sv_table <- sv_table[, nchar(getSequences(sv_table)) %in% seq(min_len, max_len)]

table(nchar(getSequences(sv_table)))

```


##Remove chimeras

DADA2 only considers "bimeras", or chimeras spawned from exactly two parents sequences.
```{r remove chimeras}

sv_table.no_chim <- removeBimeraDenovo(sv_table, method = "consensus", verbose = TRUE)

#check what percentage of reads remain
sum(sv_table.no_chim) / sum(sv_table)

```


##Track read retention through the pipeline

See how many reads were retained or discarded at each stage of processing.
```{r track reads}

getN <- function(x) sum(getUniques(x))

if (length(sample_names) > 1){
  track_table <- cbind(filt_stats, sapply(dada_Fs, getN), sapply(mergers, getN), rowSums(sv_table), rowSums(sv_table.no_chim))
} else {
  track_table <- cbind(filt_stats, getN(dada_Fs), getN(mergers), sum(sv_table), sum(sv_table.no_chim))
}

colnames(track_table) <- c("raw", "filtered", "denoised", "merged", "tabled", "non_chim")
rownames(track_table) <- sample_names
track_table

save(mergers, sv_table, sv_table.no_chim, track_table, file = file.path(dada2_path, "tables.RData"))
write.table(sv_table.no_chim, file = file.path(dada2_path, "sv_table.no_chim.txt"), quote = FALSE, sep = "\t")

```

